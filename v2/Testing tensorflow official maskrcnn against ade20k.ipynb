{"cells":[{"cell_type":"code","source":["!pip uninstall --yes opencv-python-headless"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5e9RURNiK7Co","executionInfo":{"status":"ok","timestamp":1651786918547,"user_tz":420,"elapsed":1258,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"b3fc27fe-ebba-4fc4-8d8c-d18f01fceb05"},"id":"5e9RURNiK7Co","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: opencv-python-headless 4.5.5.64\n","Uninstalling opencv-python-headless-4.5.5.64:\n","  Successfully uninstalled opencv-python-headless-4.5.5.64\n"]}]},{"cell_type":"code","source":["!pip install opencv-python-headless==4.1.2.30"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Dt5yxOELETf","executionInfo":{"status":"ok","timestamp":1651786966280,"user_tz":420,"elapsed":9542,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"d2de5dec-9da7-4572-9daf-59178a880d8a"},"id":"7Dt5yxOELETf","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencv-python-headless==4.1.2.30\n","  Downloading opencv_python_headless-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (21.8 MB)\n","\u001b[K     |████████████████████████████████| 21.8 MB 24.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.6)\n","Installing collected packages: opencv-python-headless\n","Successfully installed opencv-python-headless-4.1.2.30\n"]}]},{"cell_type":"code","execution_count":39,"id":"31b95a69","metadata":{"id":"31b95a69","executionInfo":{"status":"ok","timestamp":1651815007122,"user_tz":420,"elapsed":171,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import cv2\n","import requests\n","from PIL import Image, ImageDraw, ImageFont\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from google.colab.patches import cv2_imshow\n","pd.options.display.max_colwidth = 120\n","pd.options.display.max_rows = 120"]},{"cell_type":"markdown","id":"4e305a6e","metadata":{"id":"4e305a6e"},"source":["## Read ADE metadata"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/fourthbrain/Week 12')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NiX9XRx5R_b","executionInfo":{"status":"ok","timestamp":1651811697255,"user_tz":420,"elapsed":17398,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"3cf85f6a-4c62-45df-ff4f-b61fa4ddf7e1"},"id":"3NiX9XRx5R_b","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"id":"4159405b","metadata":{"id":"4159405b","executionInfo":{"status":"ok","timestamp":1651811699119,"user_tz":420,"elapsed":1875,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[],"source":["dfh = pd.read_csv('ade20k_header.csv')\n","dfd = pd.read_csv('ade20k_detail.csv')\n","dfd15 = pd.read_csv('ade_coco_15pts.csv')"]},{"cell_type":"code","source":["os.chdir('/content')"],"metadata":{"id":"qnFIVk39HYJA","executionInfo":{"status":"ok","timestamp":1651811699128,"user_tz":420,"elapsed":24,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"id":"qnFIVk39HYJA","execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"c27b9de7","metadata":{"id":"c27b9de7","outputId":"b0a61f36-29b2-4dfd-bc46-381d6bbb6e9c","colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"status":"ok","timestamp":1651811699130,"user_tz":420,"elapsed":23,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       header_id  width  height                filename  \\\n","7122       19927   1024     768  ADE_train_00019927.jpg   \n","11274       5035    600     416  ADE_train_00005035.jpg   \n","9487       12039   2048    1351  ADE_train_00012039.jpg   \n","22173      17513   2048    1536  ADE_train_00017513.jpg   \n","9063        9772    590     299  ADE_train_00009772.jpg   \n","\n","                                                                     folder  \\\n","7122            ADE20K_2021_17_01/images/ADE/training/home_or_hotel/wet_bar   \n","11274     ADE20K_2021_17_01/images/ADE/training/shopping_and_dining/canteen   \n","9487   ADE20K_2021_17_01/images/ADE/training/nature_landscape/mountain_path   \n","22173                    ADE20K_2021_17_01/images/ADE/training/urban/street   \n","9063        ADE20K_2021_17_01/images/ADE/training/nature_landscape/ice_floe   \n","\n","                                                  scene  \n","7122             ['indoor', 'home or hotel', 'wet_bar']  \n","11274      ['indoor', 'shopping and dining', 'canteen']  \n","9487   ['outdoor', 'nature landscape', 'mountain_path']  \n","22173                    ['outdoor', 'urban', 'street']  \n","9063        ['outdoor', 'nature landscape', 'ice_floe']  "],"text/html":["\n","  <div id=\"df-ffec7133-a468-4b12-8d9c-ce252c224f58\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>header_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>filename</th>\n","      <th>folder</th>\n","      <th>scene</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7122</th>\n","      <td>19927</td>\n","      <td>1024</td>\n","      <td>768</td>\n","      <td>ADE_train_00019927.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/home_or_hotel/wet_bar</td>\n","      <td>['indoor', 'home or hotel', 'wet_bar']</td>\n","    </tr>\n","    <tr>\n","      <th>11274</th>\n","      <td>5035</td>\n","      <td>600</td>\n","      <td>416</td>\n","      <td>ADE_train_00005035.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/shopping_and_dining/canteen</td>\n","      <td>['indoor', 'shopping and dining', 'canteen']</td>\n","    </tr>\n","    <tr>\n","      <th>9487</th>\n","      <td>12039</td>\n","      <td>2048</td>\n","      <td>1351</td>\n","      <td>ADE_train_00012039.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/nature_landscape/mountain_path</td>\n","      <td>['outdoor', 'nature landscape', 'mountain_path']</td>\n","    </tr>\n","    <tr>\n","      <th>22173</th>\n","      <td>17513</td>\n","      <td>2048</td>\n","      <td>1536</td>\n","      <td>ADE_train_00017513.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","    <tr>\n","      <th>9063</th>\n","      <td>9772</td>\n","      <td>590</td>\n","      <td>299</td>\n","      <td>ADE_train_00009772.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/nature_landscape/ice_floe</td>\n","      <td>['outdoor', 'nature landscape', 'ice_floe']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffec7133-a468-4b12-8d9c-ce252c224f58')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ffec7133-a468-4b12-8d9c-ce252c224f58 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ffec7133-a468-4b12-8d9c-ce252c224f58');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["dfh.sample(5)"]},{"cell_type":"code","execution_count":7,"id":"b1453f28","metadata":{"id":"b1453f28","outputId":"3620a990-8743-4896-ba87-36ecf7a3af1b","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1651811699131,"user_tz":420,"elapsed":21,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id    raw_name  depth_ordering_rank  crop  header_id  num_mask_points  \\\n","213427   7      window                    8     0      11393               44   \n","119517  63  right hand                   62     0       8367                6   \n","576835  63    left leg                   62     0      18594               33   \n","474628  67         leg                   68     0      15291               12   \n","107515  30       chair                   31     1       7144               32   \n","\n","        xmin  ymin  xmax  ymax  width  height  \n","213427     1   490  1459  1086   1458     596  \n","119517   361   114   366   123      5       9  \n","576835  1467  1142  1497  1260     30     118  \n","474628   434   904   477   977     43      73  \n","107515   235   406   362   591    127     185  "],"text/html":["\n","  <div id=\"df-2860eb10-f44d-4967-bd5a-72f17a409053\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>raw_name</th>\n","      <th>depth_ordering_rank</th>\n","      <th>crop</th>\n","      <th>header_id</th>\n","      <th>num_mask_points</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>xmax</th>\n","      <th>ymax</th>\n","      <th>width</th>\n","      <th>height</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>213427</th>\n","      <td>7</td>\n","      <td>window</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>11393</td>\n","      <td>44</td>\n","      <td>1</td>\n","      <td>490</td>\n","      <td>1459</td>\n","      <td>1086</td>\n","      <td>1458</td>\n","      <td>596</td>\n","    </tr>\n","    <tr>\n","      <th>119517</th>\n","      <td>63</td>\n","      <td>right hand</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>8367</td>\n","      <td>6</td>\n","      <td>361</td>\n","      <td>114</td>\n","      <td>366</td>\n","      <td>123</td>\n","      <td>5</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>576835</th>\n","      <td>63</td>\n","      <td>left leg</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>18594</td>\n","      <td>33</td>\n","      <td>1467</td>\n","      <td>1142</td>\n","      <td>1497</td>\n","      <td>1260</td>\n","      <td>30</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>474628</th>\n","      <td>67</td>\n","      <td>leg</td>\n","      <td>68</td>\n","      <td>0</td>\n","      <td>15291</td>\n","      <td>12</td>\n","      <td>434</td>\n","      <td>904</td>\n","      <td>477</td>\n","      <td>977</td>\n","      <td>43</td>\n","      <td>73</td>\n","    </tr>\n","    <tr>\n","      <th>107515</th>\n","      <td>30</td>\n","      <td>chair</td>\n","      <td>31</td>\n","      <td>1</td>\n","      <td>7144</td>\n","      <td>32</td>\n","      <td>235</td>\n","      <td>406</td>\n","      <td>362</td>\n","      <td>591</td>\n","      <td>127</td>\n","      <td>185</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2860eb10-f44d-4967-bd5a-72f17a409053')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2860eb10-f44d-4967-bd5a-72f17a409053 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2860eb10-f44d-4967-bd5a-72f17a409053');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["dfd.sample(5)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bj5t5oXLsbcO","outputId":"b4f31667-5a81-4b46-ccd9-2d77826afb86","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1651811699132,"user_tz":420,"elapsed":19,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Unnamed: 0  id raw_name  depth_ordering_rank  crop  header_id  \\\n","44556       44556   0      car                    1     0      17152   \n","11562       11562  14      bed                   15     0      20958   \n","16853       16853   5    chair                    6     0      23768   \n","39030       39030   4    truck                    5     0       9164   \n","35083       35083   1    truck                    2     0       1071   \n","\n","       num_mask_points  xmin  ymin  xmax  ymax  width  height coco_label  \n","44556               22   719   849  1680  1177    961     328        car  \n","11562               41   196   227  1280   960   1084     733        bed  \n","16853               44   337   414   441   571    104     157      chair  \n","39030               18   199   293   237   330     38      37      truck  \n","35083               18  1493  1015  1600  1115    107     100      truck  "],"text/html":["\n","  <div id=\"df-544dcd2b-6bcd-43f0-a238-496c84963f44\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>raw_name</th>\n","      <th>depth_ordering_rank</th>\n","      <th>crop</th>\n","      <th>header_id</th>\n","      <th>num_mask_points</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>xmax</th>\n","      <th>ymax</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>coco_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>44556</th>\n","      <td>44556</td>\n","      <td>0</td>\n","      <td>car</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>17152</td>\n","      <td>22</td>\n","      <td>719</td>\n","      <td>849</td>\n","      <td>1680</td>\n","      <td>1177</td>\n","      <td>961</td>\n","      <td>328</td>\n","      <td>car</td>\n","    </tr>\n","    <tr>\n","      <th>11562</th>\n","      <td>11562</td>\n","      <td>14</td>\n","      <td>bed</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>20958</td>\n","      <td>41</td>\n","      <td>196</td>\n","      <td>227</td>\n","      <td>1280</td>\n","      <td>960</td>\n","      <td>1084</td>\n","      <td>733</td>\n","      <td>bed</td>\n","    </tr>\n","    <tr>\n","      <th>16853</th>\n","      <td>16853</td>\n","      <td>5</td>\n","      <td>chair</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>23768</td>\n","      <td>44</td>\n","      <td>337</td>\n","      <td>414</td>\n","      <td>441</td>\n","      <td>571</td>\n","      <td>104</td>\n","      <td>157</td>\n","      <td>chair</td>\n","    </tr>\n","    <tr>\n","      <th>39030</th>\n","      <td>39030</td>\n","      <td>4</td>\n","      <td>truck</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>9164</td>\n","      <td>18</td>\n","      <td>199</td>\n","      <td>293</td>\n","      <td>237</td>\n","      <td>330</td>\n","      <td>38</td>\n","      <td>37</td>\n","      <td>truck</td>\n","    </tr>\n","    <tr>\n","      <th>35083</th>\n","      <td>35083</td>\n","      <td>1</td>\n","      <td>truck</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1071</td>\n","      <td>18</td>\n","      <td>1493</td>\n","      <td>1015</td>\n","      <td>1600</td>\n","      <td>1115</td>\n","      <td>107</td>\n","      <td>100</td>\n","      <td>truck</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-544dcd2b-6bcd-43f0-a238-496c84963f44')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-544dcd2b-6bcd-43f0-a238-496c84963f44 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-544dcd2b-6bcd-43f0-a238-496c84963f44');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["dfd15.sample(5)"],"id":"bj5t5oXLsbcO"},{"cell_type":"markdown","id":"5cbbcc71","metadata":{"id":"5cbbcc71"},"source":["## Grab 300 random files from dfd15. Let's call this `df300`"]},{"cell_type":"code","execution_count":9,"id":"de22b8ae","metadata":{"id":"de22b8ae","outputId":"db4b296e-0b86-460c-ae63-c0914dbe27bd","colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"status":"ok","timestamp":1651811702031,"user_tz":420,"elapsed":165,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       header_id  width  height                filename  \\\n","17946       5015   2200    1650  ADE_train_00005015.jpg   \n","21511        282    916     605  ADE_frame_00000282.jpg   \n","14001       7754   1280     960  ADE_train_00007754.jpg   \n","22911      18251   2048    1536  ADE_train_00018251.jpg   \n","23301      18641    256     256  ADE_train_00018641.jpg   \n","\n","                                                                           folder  \\\n","17946                    ADE20K_2021_17_01/images/ADE/training/urban/canal__urban   \n","21511                          ADE20K_2021_17_01/images/ADE/training/urban/street   \n","14001  ADE20K_2021_17_01/images/ADE/training/transportation/ferryboat__cargo_deck   \n","22911                          ADE20K_2021_17_01/images/ADE/training/urban/street   \n","23301                          ADE20K_2021_17_01/images/ADE/training/urban/street   \n","\n","                                                      scene  \n","17946                   ['outdoor', 'urban', 'canal urban']  \n","21511                        ['outdoor', 'urban', 'street']  \n","14001  ['indoor', 'transportation', 'ferryboat cargo_deck']  \n","22911                        ['outdoor', 'urban', 'street']  \n","23301                        ['outdoor', 'urban', 'street']  "],"text/html":["\n","  <div id=\"df-0f353845-f97b-464a-ac37-6bb2151c5306\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>header_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>filename</th>\n","      <th>folder</th>\n","      <th>scene</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17946</th>\n","      <td>5015</td>\n","      <td>2200</td>\n","      <td>1650</td>\n","      <td>ADE_train_00005015.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/canal__urban</td>\n","      <td>['outdoor', 'urban', 'canal urban']</td>\n","    </tr>\n","    <tr>\n","      <th>21511</th>\n","      <td>282</td>\n","      <td>916</td>\n","      <td>605</td>\n","      <td>ADE_frame_00000282.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","    <tr>\n","      <th>14001</th>\n","      <td>7754</td>\n","      <td>1280</td>\n","      <td>960</td>\n","      <td>ADE_train_00007754.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/transportation/ferryboat__cargo_deck</td>\n","      <td>['indoor', 'transportation', 'ferryboat cargo_deck']</td>\n","    </tr>\n","    <tr>\n","      <th>22911</th>\n","      <td>18251</td>\n","      <td>2048</td>\n","      <td>1536</td>\n","      <td>ADE_train_00018251.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","    <tr>\n","      <th>23301</th>\n","      <td>18641</td>\n","      <td>256</td>\n","      <td>256</td>\n","      <td>ADE_train_00018641.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f353845-f97b-464a-ac37-6bb2151c5306')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0f353845-f97b-464a-ac37-6bb2151c5306 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0f353845-f97b-464a-ac37-6bb2151c5306');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["sample = dfd15.header_id.sample(n=300, random_state=52).values\n","df300 = dfh.loc[dfh.header_id.isin(sample)]\n","df300.sample(5)"]},{"cell_type":"code","source":["%%bash\n","cp '/content/drive/My Drive/Colab Notebooks/fourthbrain/Week 12/df300.tar.gz' /content\n","cd /content\n","tar zxvf df300.tar.gz"],"metadata":{"id":"eLs9mpTrNr2C"},"id":"eLs9mpTrNr2C","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9c1636fd","metadata":{"id":"9c1636fd"},"source":["## Prepare tf official implementation of maskrcnn"]},{"cell_type":"markdown","source":["### Prepare environment and imports"],"metadata":{"id":"XsZ5oV3DTBcq"},"id":"XsZ5oV3DTBcq"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBYzVHyEsbcX","executionInfo":{"status":"ok","timestamp":1651811824392,"user_tz":420,"elapsed":3385,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"382fdb6a-d7e4-4545-d474-5e71726fada3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'models'...\n","remote: Enumerating objects: 3336, done.\u001b[K\n","remote: Counting objects: 100% (3336/3336), done.\u001b[K\n","remote: Compressing objects: 100% (2771/2771), done.\u001b[K\n","remote: Total 3336 (delta 882), reused 1417 (delta 515), pack-reused 0\u001b[K\n","Receiving objects: 100% (3336/3336), 34.33 MiB | 32.31 MiB/s, done.\n","Resolving deltas: 100% (882/882), done.\n"]}],"source":["import os\n","import pathlib\n","\n","if \"models\" in pathlib.Path.cwd().parts:\n","  while \"models\" in pathlib.Path.cwd().parts:\n","    os.chdir('..')\n","elif not pathlib.Path('models').exists():\n","  !git clone --depth 1 https://github.com/tensorflow/models"],"id":"qBYzVHyEsbcX"},{"cell_type":"code","execution_count":13,"metadata":{"id":"NXh51Ie3sbcZ","executionInfo":{"status":"ok","timestamp":1651811856333,"user_tz":420,"elapsed":31946,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25fcc272-ac17-437a-d8cb-54f79a3b0f3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /content/models/research\n","Collecting avro-python3\n","  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n","Collecting apache-beam\n","  Downloading apache_beam-2.38.0-cp37-cp37m-manylinux2010_x86_64.whl (10.2 MB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.28)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n","Collecting tf-slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n","Collecting lvis\n","  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n","Collecting tf-models-official>=2.5.1\n","  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n","Collecting tensorflow_io\n","  Downloading tensorflow_io-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.8.0)\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n","Collecting opencv-python-headless\n","  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n","Collecting py-cpuinfo>=3.3.0\n","  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","Collecting tensorflow-model-optimization>=0.4.1\n","  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n","Collecting tensorflow-text~=2.8.0\n","  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n","Collecting pyyaml<6.0,>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.6)\n","Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.8.0)\n","Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.5)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n","Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.1)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.8)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (2.8.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.5.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (4.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.44.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.25.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.1)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n","Requirement already satisfied: pyarrow<7.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n","Collecting requests<3.0.0dev,>=2.18.0\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","Collecting cloudpickle<3,>=2.0.0\n","  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n","Collecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","Collecting fastavro<2,>=0.23.6\n","  Downloading fastavro-1.4.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","Collecting proto-plus<2,>=1.7.1\n","  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n","Collecting pymongo<4.0.0,>=3.8.0\n","  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n","Collecting orjson<4.0\n","  Downloading orjson-3.6.8-cp37-cp37m-manylinux_2_24_x86_64.whl (253 kB)\n","Collecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n","Collecting protobuf>=3.12.0\n","  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.12)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.2)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.16.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.7.1)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.4.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.7.0)\n","Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, seqeval\n","  Building wheel for object-detection (setup.py): started\n","  Building wheel for object-detection (setup.py): finished with status 'done'\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1692480 sha256=70b4822a7f0bc2c981e00725d99c5aa7487395c661dc890624513358533b8776\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-973tvrcf/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n","  Building wheel for py-cpuinfo (setup.py): started\n","  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n","  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=3723810b035cd7976ef090c644b783a93b870aef083c944d9f75ec0be36486fd\n","  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n","  Building wheel for dill (setup.py): started\n","  Building wheel for dill (setup.py): finished with status 'done'\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=7a416e3664575312d1a77aceb03de27c65af45c8bd8a6c74cd1235a61ba2f2bb\n","  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n","  Building wheel for avro-python3 (setup.py): started\n","  Building wheel for avro-python3 (setup.py): finished with status 'done'\n","  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=f80236a3856296878aac03ddb7772ac9abbb9ba701ff582010c452de3a6aa278\n","  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n","  Building wheel for seqeval (setup.py): started\n","  Building wheel for seqeval (setup.py): finished with status 'done'\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=cf758ec5a4ede059dce055fd29137a7f02baf543ef769a52de5fa95298ee1b0a\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built object-detection py-cpuinfo dill avro-python3 seqeval\n","Installing collected packages: requests, protobuf, tf-estimator-nightly, portalocker, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, opencv-python-headless, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.4\n","    Uninstalling dill-0.3.4:\n","      Successfully uninstalled dill-0.3.4\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pymongo\n","    Found existing installation: pymongo 4.1.1\n","    Uninstalling pymongo-4.1.1:\n","      Successfully uninstalled pymongo-4.1.1\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","Successfully installed apache-beam-2.38.0 avro-python3-1.10.2 cloudpickle-2.0.0 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.11 hdfs-2.7.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.5.64 orjson-3.6.8 portalocker-2.4.0 proto-plus-1.20.3 protobuf-3.20.1 py-cpuinfo-8.0.0 pymongo-3.12.3 pyyaml-5.4.1 requests-2.27.1 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.16.1 tensorflow-io-0.25.0 tensorflow-model-optimization-0.7.2 tensorflow-text-2.8.2 tf-estimator-nightly-2.8.0.dev2021122109 tf-models-official-2.8.0 tf-slim-1.1.0\n"]},{"output_type":"stream","name":"stderr","text":["bash: line 3: cd: models/research: No such file or directory\n","  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n"]}],"source":["%%bash\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cd models/research\n","cp object_detection/packages/tf2/setup.py .\n","pip install ."],"id":"NXh51Ie3sbcZ"},{"cell_type":"code","execution_count":14,"metadata":{"id":"oApJWlFesbcc","executionInfo":{"status":"ok","timestamp":1651811859033,"user_tz":420,"elapsed":2710,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[],"source":["import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image, ImageDraw, ImageOps\n","from IPython.display import display"],"id":"oApJWlFesbcc"},{"cell_type":"code","execution_count":15,"metadata":{"id":"vpNepchXsbcd","executionInfo":{"status":"ok","timestamp":1651811859405,"user_tz":420,"elapsed":380,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[],"source":["from object_detection.utils import ops as utils_ops\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as vis_util"],"id":"vpNepchXsbcd"},{"cell_type":"markdown","source":["### Retrieve the model"],"metadata":{"id":"DqNwcRfDTJWV"},"id":"DqNwcRfDTJWV"},{"cell_type":"code","source":["model_name = 'mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8' # mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28"],"metadata":{"id":"TsCVdaLqS25m","executionInfo":{"status":"ok","timestamp":1651811878747,"user_tz":420,"elapsed":227,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"id":"TsCVdaLqS25m","execution_count":16,"outputs":[]},{"cell_type":"code","source":["!curl -O http://download.tensorflow.org/models/object_detection/tf2/20200711/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DhgdcEghOMhM","executionInfo":{"status":"ok","timestamp":1651811882257,"user_tz":420,"elapsed":3316,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"decb907f-1ca6-4583-c817-4abdc8601fb6"},"id":"DhgdcEghOMhM","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  462M  100  462M    0     0   140M      0  0:00:03  0:00:03 --:--:--  140M\n"]}]},{"cell_type":"code","source":["!tar zxvf mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikIsyHj9OTas","executionInfo":{"status":"ok","timestamp":1651811888449,"user_tz":420,"elapsed":6197,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"5b443898-f454-4d8b-d9fa-c410c10586fd"},"id":"ikIsyHj9OTas","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/ckpt-0.data-00000-of-00001\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/checkpoint\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/ckpt-0.index\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/pipeline.config\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/saved_model/\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/saved_model/saved_model.pb\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/saved_model/assets/\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/saved_model/variables/\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/saved_model/variables/variables.data-00000-of-00001\n","mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/saved_model/variables/variables.index\n"]}]},{"cell_type":"markdown","source":["## Prepare helpers"],"metadata":{"id":"0-VQcHG4JYmZ"},"id":"0-VQcHG4JYmZ"},{"cell_type":"markdown","source":["### Geometry"],"metadata":{"id":"3xELt5VXTwF0"},"id":"3xELt5VXTwF0"},{"cell_type":"code","source":["################################################################################\n","def mask2poly(mask, distance: int = 5):\n","    \"\"\"\n","    Convert segmentation mask into an xy list of polygon points [x1, y1, x2, y2, ...].\n","\n","    Parameters\n","    ----------\n","    mask : np array\n","        Object segmentation mask of an image.\n","\n","    distance : int\n","        Euclidean distance between polygon points. We use numpy.linalg.norm, whose\n","        default is L2 norm = Euclidean distance.\n","    \n","    Returns\n","    -------\n","    list\n","        The xy list of polygon points [x1, y1, x2, y2, ...].\n","    \"\"\"\n","\n","    # Get contours of the mask. Should return 1 object in a common use case.\n","    contours, hier = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Get the contours' polygons.\n","    poly = []\n","    for obj in contours:\n","        # Set sentinel to np.inf to always pick up the first point.\n","        sentinel = [np.inf, np.inf]\n","        for point in obj:\n","            # Only keep the point if it's more than the specified distance.\n","            if np.linalg.norm(sentinel-point[0]) >= distance:\n","                poly.append(int(point[0][0]))\n","                poly.append(int(point[0][1]))\n","                sentinel = point[0]\n","\n","    return poly\n","\n","################################################################################\n","def get_crop_bbox(img, bbox, padding=None):\n","    \"\"\"\n","    Return the cropped portion of the given image.\n","\n","    Parameters\n","    ----------\n","    img : np array\n","\n","    bbox : tuple\n","        Base bounding box of the crop (xmin, ymin, xmax, ymax).\n","\n","    padding : tuple of ints, or float\n","        Padding around the base bounding box.\n","        If None, no padding is added to the box.\n","        If a tuple, specify (padleft, padtop, padright, padbottom).\n","        If a float, a uniform but random padding around the box will be calculated.\n","                First, we get a base padding of: min(bboxwidth, bboxheight) * padding_pct.\n","                Next, we add to/subtract with: padding * random.uniform(0, padding_pct).\n","\n","    Returns\n","    -------\n","    np array, tuple\n","        The cropped portion of the image, and its bounding box coordinates (xmin, ymin, xmax, ymax).\n","    \"\"\"\n","\n","    # Get image shape, bbox coordinates and shape.\n","    height, width = img.shape[:2]\n","    xmin, ymin, xmax, ymax = bbox\n","    bboxwidth, bboxheight = xmax-xmin, ymax-ymin\n","\n","    # Figure out padding in number-of-pixels.\n","    if padding == None:\n","        padleft, padtop, padright, padbottom = 0, 0, 0, 0\n","\n","    else:\n","        if type(padding) == tuple:\n","            # Get requested padding.\n","            padleft, padtop, padright, padbottom = padding\n","\n","        elif type(padding) == float:\n","            # Get base padding.\n","            padding_pct = padding\n","            padding = int( min(bboxwidth, bboxheight) * padding_pct )\n","\n","            # Make random addition/subtraction from base padding.\n","            padleft, padtop, padright, padbottom = padding * (1 + np.random.uniform(low=-padding_pct, high=padding_pct, size=4))\n","\n","        # Figure out extra room after requested padding.\n","        extraleft, extraright = xmin-padleft, width-xmax-padright\n","        extratop, extrabottom = ymin-padtop, height-ymax-padbottom\n","\n","        # Center bbox if not enough room horizontally.\n","        padhorz = max(0, extraleft+extraright)//2\n","        padleft, padright = (padhorz, padhorz) if min(extraleft, extraright) < 0 else (padleft, padright)\n","\n","        # Do the same vertically.\n","        padvert = max(0, extratop+extrabottom)//2\n","        padtop, padbottom = (padvert, padvert) if min(extratop, extrabottom) < 0 else (padtop, padbottom)\n","\n","    # Calculate final bbox.\n","    xmin, xmax = int(xmin-padleft), int(xmax+padright)\n","    ymin, ymax = int(ymin-padtop), int(ymax+padbottom)\n","\n","    return img[ymin:ymax, xmin:xmax], (xmin, ymin, xmax, ymax)"],"metadata":{"id":"bimKzwCJJcGd","executionInfo":{"status":"ok","timestamp":1651811903030,"user_tz":420,"elapsed":157,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"id":"bimKzwCJJcGd","execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Inference"],"metadata":{"id":"fFC762zfT0kf"},"id":"fFC762zfT0kf"},{"cell_type":"code","source":["################################################################################\n","def run_inference_for_single_image(model, image):\n","  # Convert to np array.\n","  image_np = np.array(image)\n","\n","  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n","  input_tensor = tf.convert_to_tensor(image_np)\n","  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n","  input_tensor = input_tensor[tf.newaxis,...]\n","  \n","  # Run inference.\n","  model_fn = model.signatures['serving_default']\n","  output_dict = model_fn(input_tensor)\n","\n","  return image_np, input_tensor, output_dict\n","\n","################################################################################\n","def get_predictions(output_dict):\n","  num_detections = int(output_dict['num_detections'])\n","  predictions = {}\n","  predictions['num_detections'] = num_detections\n","  for k in ['boxes', 'classes', 'masks', 'scores']:\n","    key = 'detection_{}'.format(k)\n","    predictions[key] = output_dict[key][0, :num_detections].numpy()\n","  \n","  predictions['detection_classes'] = predictions['detection_classes'].astype(np.int64)\n","  \n","  if 'detection_masks' in output_dict:\n","    # Reframe the the bbox mask to the image size.\n","    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","      predictions['detection_masks'],\n","      predictions['detection_boxes'],\n","      image_np.shape[0],\n","      image_np.shape[1]\n","    )\n","    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n","    predictions['detection_masks_reframed'] = detection_masks_reframed.numpy()\n","\n","  return predictions"],"metadata":{"id":"R80Upx8gRnE-","executionInfo":{"status":"ok","timestamp":1651811911296,"user_tz":420,"elapsed":153,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"id":"R80Upx8gRnE-","execution_count":20,"outputs":[]},{"cell_type":"markdown","id":"315a5d61","metadata":{"id":"315a5d61"},"source":["## Try inference on some images from df300"]},{"cell_type":"code","source":["df300.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"lSXc6zaY6Vev","executionInfo":{"status":"ok","timestamp":1651681882037,"user_tz":420,"elapsed":173,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"7c64c69f-e33f-4eb9-cba9-4c66590b8863"},"id":"lSXc6zaY6Vev","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     header_id                filename  \\\n","147      22687  ADE_train_00022687.jpg   \n","198       1742  ADE_train_00001742.jpg   \n","359       2007  ADE_train_00002007.jpg   \n","458       5391  ADE_train_00005391.jpg   \n","501        106  ADE_frame_00000106.jpg   \n","\n","                                                         folder  \\\n","147  ADE20K_2021_17_01/images/ADE/training/cultural/art_gallery   \n","198   ADE20K_2021_17_01/images/ADE/training/cultural/art_studio   \n","359   ADE20K_2021_17_01/images/ADE/training/cultural/auditorium   \n","458       ADE20K_2021_17_01/images/ADE/training/cultural/chapel   \n","501    ADE20K_2021_17_01/images/ADE/training/cultural/classroom   \n","\n","              imsize                                  scene  \n","147   [750, 1000, 3]  ['indoor', 'cultural', 'art_gallery']  \n","198    [311, 478, 3]   ['indoor', 'cultural', 'art_studio']  \n","359   [768, 1024, 3]   ['indoor', 'cultural', 'auditorium']  \n","458  [1200, 1600, 3]       ['indoor', 'cultural', 'chapel']  \n","501  [1467, 2200, 3]    ['indoor', 'cultural', 'classroom']  "],"text/html":["\n","  <div id=\"df-4e025721-9bf5-4246-ae25-84e6062ef889\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>header_id</th>\n","      <th>filename</th>\n","      <th>folder</th>\n","      <th>imsize</th>\n","      <th>scene</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>147</th>\n","      <td>22687</td>\n","      <td>ADE_train_00022687.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/cultural/art_gallery</td>\n","      <td>[750, 1000, 3]</td>\n","      <td>['indoor', 'cultural', 'art_gallery']</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>1742</td>\n","      <td>ADE_train_00001742.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/cultural/art_studio</td>\n","      <td>[311, 478, 3]</td>\n","      <td>['indoor', 'cultural', 'art_studio']</td>\n","    </tr>\n","    <tr>\n","      <th>359</th>\n","      <td>2007</td>\n","      <td>ADE_train_00002007.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/cultural/auditorium</td>\n","      <td>[768, 1024, 3]</td>\n","      <td>['indoor', 'cultural', 'auditorium']</td>\n","    </tr>\n","    <tr>\n","      <th>458</th>\n","      <td>5391</td>\n","      <td>ADE_train_00005391.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/cultural/chapel</td>\n","      <td>[1200, 1600, 3]</td>\n","      <td>['indoor', 'cultural', 'chapel']</td>\n","    </tr>\n","    <tr>\n","      <th>501</th>\n","      <td>106</td>\n","      <td>ADE_frame_00000106.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/cultural/classroom</td>\n","      <td>[1467, 2200, 3]</td>\n","      <td>['indoor', 'cultural', 'classroom']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e025721-9bf5-4246-ae25-84e6062ef889')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e025721-9bf5-4246-ae25-84e6062ef889 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e025721-9bf5-4246-ae25-84e6062ef889');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n","category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n","\n","# Load the model.\n","model_dir = '{}/saved_model'.format(model_name)\n","model = tf.saved_model.load(str(model_dir))"],"metadata":{"id":"F3lFfXuRSlip","executionInfo":{"status":"ok","timestamp":1651811974368,"user_tz":420,"elapsed":49279,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"id":"F3lFfXuRSlip","execution_count":21,"outputs":[]},{"cell_type":"code","source":["df300.loc[df300.width > 900].sample(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"id":"JaccbLI0VKIt","executionInfo":{"status":"ok","timestamp":1651811986172,"user_tz":420,"elapsed":169,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"4ec37708-3fcd-400a-864a-0e4ea91edd8c"},"id":"JaccbLI0VKIt","execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       header_id  width  height                filename  \\\n","2126         305   2200    1650  ADE_train_00000305.jpg   \n","22631      17971   2048    1536  ADE_train_00017971.jpg   \n","22102      17442   2048    1536  ADE_train_00017442.jpg   \n","22273      17613   2200    1650  ADE_train_00017613.jpg   \n","2240         419   1024     768  ADE_train_00000419.jpg   \n","\n","                                                            folder  \\\n","2126   ADE20K_2021_17_01/images/ADE/training/home_or_hotel/bedroom   \n","22631           ADE20K_2021_17_01/images/ADE/training/urban/street   \n","22102           ADE20K_2021_17_01/images/ADE/training/urban/street   \n","22273           ADE20K_2021_17_01/images/ADE/training/urban/street   \n","2240   ADE20K_2021_17_01/images/ADE/training/home_or_hotel/bedroom   \n","\n","                                        scene  \n","2126   ['indoor', 'home or hotel', 'bedroom']  \n","22631          ['outdoor', 'urban', 'street']  \n","22102          ['outdoor', 'urban', 'street']  \n","22273          ['outdoor', 'urban', 'street']  \n","2240   ['indoor', 'home or hotel', 'bedroom']  "],"text/html":["\n","  <div id=\"df-cbce7357-e04f-48df-a5fd-634f8ded455d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>header_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>filename</th>\n","      <th>folder</th>\n","      <th>scene</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2126</th>\n","      <td>305</td>\n","      <td>2200</td>\n","      <td>1650</td>\n","      <td>ADE_train_00000305.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/home_or_hotel/bedroom</td>\n","      <td>['indoor', 'home or hotel', 'bedroom']</td>\n","    </tr>\n","    <tr>\n","      <th>22631</th>\n","      <td>17971</td>\n","      <td>2048</td>\n","      <td>1536</td>\n","      <td>ADE_train_00017971.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","    <tr>\n","      <th>22102</th>\n","      <td>17442</td>\n","      <td>2048</td>\n","      <td>1536</td>\n","      <td>ADE_train_00017442.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","    <tr>\n","      <th>22273</th>\n","      <td>17613</td>\n","      <td>2200</td>\n","      <td>1650</td>\n","      <td>ADE_train_00017613.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/urban/street</td>\n","      <td>['outdoor', 'urban', 'street']</td>\n","    </tr>\n","    <tr>\n","      <th>2240</th>\n","      <td>419</td>\n","      <td>1024</td>\n","      <td>768</td>\n","      <td>ADE_train_00000419.jpg</td>\n","      <td>ADE20K_2021_17_01/images/ADE/training/home_or_hotel/bedroom</td>\n","      <td>['indoor', 'home or hotel', 'bedroom']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbce7357-e04f-48df-a5fd-634f8ded455d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cbce7357-e04f-48df-a5fd-634f8ded455d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cbce7357-e04f-48df-a5fd-634f8ded455d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":40,"id":"f3f75fcc","metadata":{"id":"f3f75fcc","executionInfo":{"status":"ok","timestamp":1651815576616,"user_tz":420,"elapsed":999,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}}},"outputs":[],"source":["# Choose an image.\n","header_id = 17971\n","image_path = '/content/df300/' + df300.loc[df300.header_id==header_id].filename.values[0]\n","#image = Image.open(image_path)\n","image = mpimg.imread(image_path)\n","\n","# Run inference.\n","image_np, input_tensor, output_dict = run_inference_for_single_image(model, image)"]},{"cell_type":"code","source":["# Get relevant predictions and display.\n","preds = get_predictions(output_dict)\n","vis_util.visualize_boxes_and_labels_on_image_array(\n","    image_np,\n","    preds['detection_boxes'],\n","    preds['detection_classes'],\n","    preds['detection_scores'],\n","    category_index,\n","    instance_masks=preds.get('detection_masks_reframed', None),\n","    use_normalized_coordinates=True,\n","    line_thickness=1\n",")\n","\n","display(Image.fromarray(image_np))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":848,"output_embedded_package_id":"1JysWh4iClXg-nWGkmEVz7VS8ifxNJ1Qr"},"id":"A7C3ks9QUVkL","executionInfo":{"status":"ok","timestamp":1651815584910,"user_tz":420,"elapsed":6474,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"f239cbe2-b41e-4384-ad0a-e1e64825b48c"},"id":"A7C3ks9QUVkL","execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["print(preds.keys())\n","preds['detection_classes']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Tw8CSUe0OJ2","executionInfo":{"status":"ok","timestamp":1651815605165,"user_tz":420,"elapsed":182,"user":{"displayName":"Juswaldy Jusman","userId":"04395087425696224653"}},"outputId":"23d72da6-3d38-4669-fd73-223f05d3bd53"},"id":"-Tw8CSUe0OJ2","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['num_detections', 'detection_boxes', 'detection_classes', 'detection_masks', 'detection_scores', 'detection_masks_reframed'])\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 3, 10,  8, 64, 10, 10,  1, 64,  1, 10,  1, 10,  1,  1,  1,  1,  1,\n","        1,  1,  1,  1,  1,  1, 10,  1,  1, 10, 64, 10,  1,  1,  1,  1,  1,\n","       64,  1,  1,  1,  1,  1,  1,  1, 85,  1, 64,  1, 10,  1,  1,  1,  1,\n","        1,  3,  1, 31, 31, 64,  1, 10, 10, 10,  1,  1, 10,  1, 28, 86,  1,\n","        1,  1,  1,  1,  1,  1, 10,  1,  1, 42, 27, 31, 31, 10,  1,  1, 27,\n","        1, 31,  1,  1,  1,  1,  1,  1,  1, 15,  1,  8, 13,  1, 10])"]},"metadata":{},"execution_count":42}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"Testing tensorflow official maskrcnn against ade20k.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":["4e305a6e","5cbbcc71","XsZ5oV3DTBcq","DqNwcRfDTJWV","0-VQcHG4JYmZ","3xELt5VXTwF0","fFC762zfT0kf"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}